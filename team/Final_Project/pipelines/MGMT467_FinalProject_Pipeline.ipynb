{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8ExrBkMTAO9gY54eZ8v/S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiattn/CS193_Fall18_Lab1/blob/master/team/Final_Project/pipelines/MGMT467_FinalProject_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1 - Batch Ingestion:**"
      ],
      "metadata": {
        "id": "wZa1wAPCaj4q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1rpQK39EZVu",
        "outputId": "bc0ac424-be37-451e-e708-71aa4ce36bdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries installed and Google Cloud successfully authenticated!\n"
          ]
        }
      ],
      "source": [
        "# @title Step 1.1: Setup & Authentication\n",
        "# Install required libraries for Google Cloud and Kaggle\n",
        "!pip install -q kaggle google-cloud-bigquery google-cloud-storage pandas db-dtypes\n",
        "\n",
        "# Authenticate User for Google Cloud access\n",
        "# This will trigger a popup to allow access to your GCP resources\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "print(\"Libraries installed and Google Cloud successfully authenticated!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1.2: Configuration & Kaggle JSON Upload\n",
        "import os\n",
        "import json\n",
        "from google.colab import files\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "\n",
        "# --- Google Cloud Config ---\n",
        "# User Input for Project ID\n",
        "project_id = \"mgmt-467-nh\" # @param {type:\"string\"}\n",
        "region = \"us-central1\" # @param {type:\"string\"}\n",
        "\n",
        "# Define Resource Names\n",
        "bucket_name = f\"air_quality_raw_{project_id}\" # Unique bucket name\n",
        "dataset_name = \"air_quality_dataset\"\n",
        "table_name = \"sensor_data\"\n",
        "\n",
        "# Set the environment variable for the project\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = project_id\n",
        "\n",
        "# Initialize Clients\n",
        "bq_client = bigquery.Client(project=project_id)\n",
        "storage_client = storage.Client(project=project_id)\n",
        "\n",
        "print(f\"Configuration set for Project: {project_id}\")\n",
        "print(f\"Target Bucket: {bucket_name}\")\n",
        "print(\"-------------------------------------------------\")\n",
        "\n",
        "# --- Kaggle Authentication ---\n",
        "print(\"Please upload your kaggle.json file now...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Check if kaggle.json was uploaded\n",
        "if 'kaggle.json' in uploaded:\n",
        "    # Create the .kaggle directory if it doesn't exist\n",
        "    !mkdir -p ~/.kaggle\n",
        "\n",
        "    # Move the uploaded file to the .kaggle directory\n",
        "    !mv kaggle.json ~/.kaggle/\n",
        "\n",
        "    # Change permissions to ensure the file is secure (required by Kaggle API)\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "    print(\"\\nSUCCESS: kaggle.json uploaded and permissions set.\")\n",
        "else:\n",
        "    print(\"\\nERROR: kaggle.json not found. Please run the cell again and upload the correct file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "lwiqQfMkFbRt",
        "outputId": "624b3627-bed6-4a71-f75c-ef2e03b57538"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration set for Project: mgmt-467-nh\n",
            "Target Bucket: air_quality_raw_mgmt-467-nh\n",
            "-------------------------------------------------\n",
            "Please upload your kaggle.json file now...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fdd52711-44d8-4aea-b588-20a8ada67046\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fdd52711-44d8-4aea-b588-20a8ada67046\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "\n",
            "SUCCESS: kaggle.json uploaded and permissions set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1.3: Ingest from Kaggle to GCS (Raw Data Lake)\n",
        "from google.cloud import storage\n",
        "import os\n",
        "\n",
        "# 1. Download Dataset from Kaggle\n",
        "print(\"Downloading data from Kaggle...\")\n",
        "!kaggle datasets download -d fedesoriano/air-quality-data-set\n",
        "\n",
        "# 2. Unzip the file\n",
        "print(\"Unzipping data...\")\n",
        "!unzip -o air-quality-data-set.zip\n",
        "\n",
        "# Find the .csv file name (it varies sometimes)\n",
        "files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
        "if not files:\n",
        "    raise ValueError(\"No CSV file found in the downloaded dataset!\")\n",
        "source_file_name = files[0]\n",
        "print(f\"Found raw file: {source_file_name}\")\n",
        "\n",
        "# 3. Create GCS Bucket (if it doesn't exist)\n",
        "bucket = storage_client.bucket(bucket_name)\n",
        "if not bucket.exists():\n",
        "    print(f\"Creating bucket {bucket_name}...\")\n",
        "    bucket = storage_client.create_bucket(bucket_name, location=region)\n",
        "else:\n",
        "    print(f\"Bucket {bucket_name} already exists.\")\n",
        "\n",
        "# 4. Upload File to GCS\n",
        "blob_name = source_file_name # Keep the same name in GCS\n",
        "blob = bucket.blob(blob_name)\n",
        "\n",
        "print(f\"Uploading {source_file_name} to gs://{bucket_name}/{blob_name}...\")\n",
        "blob.upload_from_filename(source_file_name)\n",
        "\n",
        "print(\"Success! Raw data is now stored in Google Cloud Storage.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4ZbzXNYGfZF",
        "outputId": "f3a77ba6-3917-4fc2-b6ef-2230785d8243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from Kaggle...\n",
            "Dataset URL: https://www.kaggle.com/datasets/fedesoriano/air-quality-data-set\n",
            "License(s): copyright-authors\n",
            "Downloading air-quality-data-set.zip to /content\n",
            "  0% 0.00/248k [00:00<?, ?B/s]\n",
            "100% 248k/248k [00:00<00:00, 518MB/s]\n",
            "Unzipping data...\n",
            "Archive:  air-quality-data-set.zip\n",
            "  inflating: AirQuality.csv          \n",
            "Found raw file: AirQuality.csv\n",
            "Creating bucket air_quality_raw_mgmt-467-nh...\n",
            "Uploading AirQuality.csv to gs://air_quality_raw_mgmt-467-nh/AirQuality.csv...\n",
            "Success! Raw data is now stored in Google Cloud Storage.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1.4 (Retry): Curated Load to BigQuery (Schema & Partitioning)\n",
        "import pandas as pd\n",
        "import re\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# 1. Read Raw Data from GCS\n",
        "source_uri = f\"gs://{bucket_name}/{source_file_name}\"\n",
        "print(f\"Reading raw data from {source_uri}...\")\n",
        "df = pd.read_csv(source_uri, sep=';', decimal=',')\n",
        "\n",
        "# 2. Data Curation\n",
        "# Drop empty columns (artifacts)\n",
        "df = df.dropna(axis=1, how='all')\n",
        "\n",
        "# Fix Date Format for Partitioning (DD/MM/YYYY -> YYYY-MM-DD)\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y').dt.date\n",
        "\n",
        "# --- FIX: Robust Column Renaming ---\n",
        "# Function to clean column names for BigQuery (only Alphanumeric and _)\n",
        "def clean_col_name(name):\n",
        "    # Replace non-alphanumeric characters (like . or () ) with _\n",
        "    clean = re.sub(r'[^a-zA-Z0-9]', '_', name)\n",
        "    return clean\n",
        "\n",
        "# Apply cleaning\n",
        "df.columns = [clean_col_name(c) for c in df.columns]\n",
        "\n",
        "print(\"Cleaned Column Names:\", df.columns.tolist())\n",
        "\n",
        "# 3. Define BigQuery Schema & Partitioning\n",
        "# We map the specific clean names to types\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    schema=[\n",
        "        bigquery.SchemaField(\"Date\", \"DATE\"),\n",
        "        bigquery.SchemaField(\"Time\", \"STRING\"),\n",
        "        # We explicitly map the cleaned names:\n",
        "        bigquery.SchemaField(\"CO_GT_\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"PT08_S1_CO_\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"NMHC_GT_\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"C6H6_GT_\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"PT08_S2_NMHC_\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"NOx_GT_\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"PT08_S3_NOx_\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"NO2_GT_\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"PT08_S4_NO2_\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"PT08_S5_O3_\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"T\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"RH\", \"FLOAT\"),\n",
        "        bigquery.SchemaField(\"AH\", \"FLOAT\"),\n",
        "    ],\n",
        "    # --- PARTITIONING ---\n",
        "    time_partitioning=bigquery.TimePartitioning(\n",
        "        type_=bigquery.TimePartitioningType.DAY,\n",
        "        field=\"Date\"\n",
        "    ),\n",
        "    write_disposition=\"WRITE_TRUNCATE\",\n",
        ")\n",
        "\n",
        "# 4. Load to BigQuery\n",
        "dataset_ref = bq_client.dataset(dataset_name)\n",
        "table_ref = dataset_ref.table(table_name)\n",
        "\n",
        "# Ensure dataset exists\n",
        "try:\n",
        "    bq_client.get_dataset(dataset_ref)\n",
        "except:\n",
        "    bq_client.create_dataset(dataset_ref)\n",
        "\n",
        "print(f\"Loading data into {project_id}.{dataset_name}.{table_name}...\")\n",
        "job = bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
        "job.result()\n",
        "\n",
        "print(\"Success! Data loaded with Cleaned Schema and Partitioning.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1kdarYnG44i",
        "outputId": "ca998aae-05ea-4440-ed99-1195f5877e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading raw data from gs://air_quality_raw_mgmt-467-nh/AirQuality.csv...\n",
            "Cleaned Column Names: ['Date', 'Time', 'CO_GT_', 'PT08_S1_CO_', 'NMHC_GT_', 'C6H6_GT_', 'PT08_S2_NMHC_', 'NOx_GT_', 'PT08_S3_NOx_', 'NO2_GT_', 'PT08_S4_NO2_', 'PT08_S5_O3_', 'T', 'RH', 'AH']\n",
            "Loading data into mgmt-467-nh.air_quality_dataset.sensor_data...\n",
            "Success! Data loaded with Cleaned Schema and Partitioning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1.5: Data Quality Check\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# --- 1. Data Quality Check (SQL) ---\n",
        "# We check for the specific error code '-200' which indicates sensor failure/missing data\n",
        "dq_query = f\"\"\"\n",
        "    SELECT\n",
        "        COUNT(*) as total_rows,\n",
        "        COUNTIF(CO_GT_ = -200) as missing_co_readings,\n",
        "        ROUND((COUNTIF(CO_GT_ = -200) / COUNT(*)) * 100, 2) as percent_missing\n",
        "    FROM `{project_id}.{dataset_name}.{table_name}`\n",
        "\"\"\"\n",
        "\n",
        "print(\"Running Data Quality Check on BigQuery...\")\n",
        "query_job = bq_client.query(dq_query)\n",
        "results = query_job.result()\n",
        "\n",
        "print(\"\\n--- Data Quality Results ---\")\n",
        "for row in results:\n",
        "    print(f\"Total Rows Loaded: {row.total_rows}\")\n",
        "    print(f\"Rows with Missing CO Data (-200): {row.missing_co_readings}\")\n",
        "    print(f\"Data Quality Impact: {row.percent_missing}% of Carbon Monoxide readings are missing.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ho1Ibw1LVhIa",
        "outputId": "f7f236be-3270-4e8e-a8cb-a13a9ce6d468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Data Quality Check on BigQuery...\n",
            "\n",
            "--- Data Quality Results ---\n",
            "Total Rows Loaded: 9471\n",
            "Rows with Missing CO Data (-200): 1683\n",
            "Data Quality Impact: 17.77% of Carbon Monoxide readings are missing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation Logic Explanation\n",
        "\n",
        "Transformation: Date Format Conversion (DD/MM/YYYY -> YYYY-MM-DD)\n",
        "\n",
        "Logic:\n",
        "The raw CSV provided dates as strings in European format (e.g., '10/03/2004').\n",
        "However, BigQuery 'Time Partitioning' strictly requires a DATE or TIMESTAMP data type.\n",
        "We parsed the string into a Python Date object during the Pandas load step.\n",
        "\n",
        "Benefit:\n",
        "By transforming this column into a DATE type, we enabled BigQuery to physically\n",
        "partition the storage by Day. This reduces query costs and improves performance\n",
        "when filtering by specific dates, as the engine only scans relevant partitions\n",
        "rather than the entire table."
      ],
      "metadata": {
        "id": "grvwIVRzV4Q9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2 - Streaming Ingestion:**"
      ],
      "metadata": {
        "id": "bw1UIoXGapx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2.1: Configure gcloud, Enable APIs & Create Pub/Sub\n",
        "# 1. Set the project for gcloud commands\n",
        "!gcloud config set project $project_id\n",
        "\n",
        "# 2. Enable APIs\n",
        "print(\"Enabling necessary APIs (this may take a minute)...\")\n",
        "!gcloud services enable \\\n",
        "    cloudfunctions.googleapis.com \\\n",
        "    run.googleapis.com \\\n",
        "    pubsub.googleapis.com \\\n",
        "    cloudbuild.googleapis.com \\\n",
        "    artifactregistry.googleapis.com --project=$project_id\n",
        "\n",
        "print(\"APIs enabled.\")\n",
        "\n",
        "# 3. Create Pub/Sub Topic\n",
        "topic_id = \"openaq-topic\"\n",
        "\n",
        "# Create the topic (using quiet flag to suppress 'already exists' error cleanly)\n",
        "!gcloud pubsub topics create $topic_id --project=$project_id || echo \"Topic likely already exists\"\n",
        "\n",
        "print(f\"\\nTarget Topic: projects/{project_id}/topics/{topic_id}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV3-MuYFaylO",
        "outputId": "467c221e-1316-4998-eb1a-21d985223aff"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n",
            "\n",
            "\n",
            "To take a quick anonymous survey, run:\n",
            "  $ gcloud survey\n",
            "\n",
            "Enabling necessary APIs (this may take a minute)...\n",
            "Operation \"operations/acat.p2-16331834856-0abb5cd4-0622-4f59-982f-13d842507270\" finished successfully.\n",
            "APIs enabled.\n",
            "Created topic [projects/mgmt-467-nh/topics/openaq-topic].\n",
            "\n",
            "Target Topic: projects/mgmt-467-nh/topics/openaq-topic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2.2: Create Streaming Table & BigQuery Subscription\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# --- 1. Create BigQuery Streaming Table ---\n",
        "streaming_table_name = \"streaming_air_quality\"\n",
        "table_ref = bq_client.dataset(dataset_name).table(streaming_table_name)\n",
        "\n",
        "# Define Schema (Data fields only)\n",
        "schema = [\n",
        "    bigquery.SchemaField(\"timestamp\", \"TIMESTAMP\"),\n",
        "    bigquery.SchemaField(\"city\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"parameter\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"value\", \"FLOAT\"),\n",
        "    bigquery.SchemaField(\"unit\", \"STRING\"),\n",
        "]\n",
        "\n",
        "table = bigquery.Table(table_ref, schema=schema)\n",
        "\n",
        "# Create table if it doesn't exist\n",
        "try:\n",
        "    bq_client.get_table(table_ref)\n",
        "    print(f\"Table {streaming_table_name} already exists.\")\n",
        "except:\n",
        "    print(f\"Creating table {streaming_table_name}...\")\n",
        "    bq_client.create_table(table)\n",
        "\n",
        "# --- 2. Grant Permissions ---\n",
        "print(\"Retrieving Project Number...\")\n",
        "project_number_list = !gcloud projects list --filter=\"project_id:$project_id\" --format=\"value(projectNumber)\"\n",
        "project_number = project_number_list[0]\n",
        "service_account = f\"service-{project_number}@gcp-sa-pubsub.iam.gserviceaccount.com\"\n",
        "\n",
        "# Grant permission\n",
        "!gcloud projects add-iam-policy-binding $project_id \\\n",
        "    --member=\"serviceAccount:$service_account\" \\\n",
        "    --role=\"roles/bigquery.dataEditor\" > /dev/null\n",
        "\n",
        "# --- 3. Create Subscription (Data Only) ---\n",
        "subscription_id = \"openaq-to-bq\"\n",
        "full_table_id = f\"{project_id}:{dataset_name}.{streaming_table_name}\"\n",
        "\n",
        "print(f\"Creating Subscription {subscription_id}...\")\n",
        "\n",
        "# We removed '--write-metadata' so it matches our table schema perfectly\n",
        "!gcloud pubsub subscriptions create $subscription_id \\\n",
        "    --topic=$topic_id \\\n",
        "    --bigquery-table=$full_table_id \\\n",
        "    --use-table-schema \\\n",
        "    --project=$project_id || echo \"Subscription likely already exists.\"\n",
        "\n",
        "print(f\"\\nPipeline Ready: Topic -> Subscription -> Table ({streaming_table_name})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aU7Ha7TNcRt5",
        "outputId": "dbdcb773-e86b-41dc-e432-2138add79fd6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Table streaming_air_quality already exists.\n",
            "Retrieving Project Number...\n",
            "Updated IAM policy for project [mgmt-467-nh].\n",
            "Creating Subscription openaq-to-bq...\n",
            "Created subscription [projects/mgmt-467-nh/subscriptions/openaq-to-bq].\n",
            "\n",
            "Pipeline Ready: Topic -> Subscription -> Table (streaming_air_quality)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2.3: Deploy Open-Meteo Cloud Function (Rome, Italy)\n",
        "import os\n",
        "\n",
        "# 1. Create Directory\n",
        "os.makedirs(\"openaq_function\", exist_ok=True)\n",
        "\n",
        "# 2. Write Function Code (Open-Meteo)\n",
        "main_py_content = \"\"\"import functions_framework\n",
        "import json\n",
        "import requests\n",
        "import os\n",
        "import datetime\n",
        "from google.cloud import pubsub_v1\n",
        "\n",
        "publisher = pubsub_v1.PublisherClient()\n",
        "PROJECT_ID = os.environ.get(\"GCP_PROJECT\")\n",
        "TOPIC_ID = \"openaq-topic\"\n",
        "topic_path = publisher.topic_path(PROJECT_ID, TOPIC_ID)\n",
        "\n",
        "@functions_framework.http\n",
        "def fetch_openaq(request):\n",
        "    \\\"\\\"\\\"\n",
        "    Fetches live Air Quality data for Rome, Italy from Open-Meteo.\n",
        "    \\\"\\\"\\\"\n",
        "    # Open-Meteo Air Quality API\n",
        "    url = \"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
        "\n",
        "    # Coordinates for Rome, Italy\n",
        "    params = {\n",
        "        \"latitude\": 41.9028,\n",
        "        \"longitude\": 12.4964,\n",
        "        \"current\": \"carbon_monoxide,nitrogen_dioxide,ozone\",\n",
        "        \"timezone\": \"Europe/Rome\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        if response.status_code != 200:\n",
        "            return f\"API Error: {response.status_code}\", 500\n",
        "\n",
        "        data = response.json()\n",
        "        current = data.get(\"current\", {})\n",
        "        current_units = data.get(\"current_units\", {})\n",
        "\n",
        "        # Map Open-Meteo names to our standard names\n",
        "        # carbon_monoxide -> co\n",
        "        # nitrogen_dioxide -> no2\n",
        "        # ozone -> o3\n",
        "        mapping = {\n",
        "            \"carbon_monoxide\": \"co\",\n",
        "            \"nitrogen_dioxide\": \"no2\",\n",
        "            \"ozone\": \"o3\"\n",
        "        }\n",
        "\n",
        "        count = 0\n",
        "\n",
        "        # Loop through the 3 gases we requested\n",
        "        for api_name, standard_name in mapping.items():\n",
        "            val = current.get(api_name)\n",
        "            unit = current_units.get(api_name, \"unknown\")\n",
        "\n",
        "            if val is not None:\n",
        "                payload = {\n",
        "                    # Use current UTC time or the time provided by API\n",
        "                    \"timestamp\": current.get(\"time\") + \":00\", # Add seconds for BQ Timestamp format\n",
        "                    \"city\": \"Rome\",\n",
        "                    \"parameter\": standard_name,\n",
        "                    \"value\": float(val),\n",
        "                    \"unit\": unit\n",
        "                }\n",
        "\n",
        "                # Publish\n",
        "                data_str = json.dumps(payload)\n",
        "                publisher.publish(topic_path, data_str.encode(\"utf-8\"))\n",
        "                count += 1\n",
        "\n",
        "        return f\"Success: Published {count} live readings from Rome.\", 200\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\", 500\n",
        "\"\"\"\n",
        "\n",
        "with open(\"openaq_function/main.py\", \"w\") as f:\n",
        "    f.write(main_py_content)\n",
        "\n",
        "# 3. Write Requirements\n",
        "with open(\"openaq_function/requirements.txt\", \"w\") as f:\n",
        "    f.write(\"functions-framework==3.*\\ngoogle-cloud-pubsub\\nrequests\\n\")\n",
        "\n",
        "# 4. Deploy\n",
        "print(\"Deploying Open-Meteo Function (Rome)...\")\n",
        "!gcloud functions deploy openaq-ingest \\\n",
        "    --gen2 \\\n",
        "    --runtime=python310 \\\n",
        "    --region=$region \\\n",
        "    --source=./openaq_function \\\n",
        "    --entry-point=fetch_openaq \\\n",
        "    --trigger-http \\\n",
        "    --allow-unauthenticated \\\n",
        "    --set-env-vars=GCP_PROJECT=$project_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdQuOpMGiOaO",
        "outputId": "ea9e0a86-61e5-4744-f105-c8a5be6d22dc"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deploying Open-Meteo Function (Rome)...\n",
            "  [INFO] A new revision will be deployed serving with 100% traffic.\n",
            "You can view your function in the Cloud Console here: https://console.cloud.google.com/functions/details/us-central1/openaq-ingest?project=mgmt-467-nh\n",
            "\n",
            "buildConfig:\n",
            "  automaticUpdatePolicy: {}\n",
            "  build: projects/16331834856/locations/us-central1/builds/4a06a525-82db-4401-94b8-df1d133dc5e8\n",
            "  dockerRegistry: ARTIFACT_REGISTRY\n",
            "  dockerRepository: projects/mgmt-467-nh/locations/us-central1/repositories/gcf-artifacts\n",
            "  entryPoint: fetch_openaq\n",
            "  runtime: python310\n",
            "  serviceAccount: projects/mgmt-467-nh/serviceAccounts/16331834856-compute@developer.gserviceaccount.com\n",
            "  source:\n",
            "    storageSource:\n",
            "      bucket: gcf-v2-sources-16331834856-us-central1\n",
            "      generation: '1764904263376193'\n",
            "      object: openaq-ingest/function-source.zip\n",
            "  sourceProvenance:\n",
            "    resolvedStorageSource:\n",
            "      bucket: gcf-v2-sources-16331834856-us-central1\n",
            "      generation: '1764904263376193'\n",
            "      object: openaq-ingest/function-source.zip\n",
            "createTime: '2025-12-04T22:50:25.191586009Z'\n",
            "environment: GEN_2\n",
            "labels:\n",
            "  deployment-tool: cli-gcloud\n",
            "name: projects/mgmt-467-nh/locations/us-central1/functions/openaq-ingest\n",
            "satisfiesPzi: true\n",
            "serviceConfig:\n",
            "  allTrafficOnLatestRevision: true\n",
            "  availableCpu: '0.1666'\n",
            "  availableMemory: 256M\n",
            "  environmentVariables:\n",
            "    GCP_PROJECT: mgmt-467-nh\n",
            "    LOG_EXECUTION_ID: 'true'\n",
            "  ingressSettings: ALLOW_ALL\n",
            "  maxInstanceCount: 100\n",
            "  maxInstanceRequestConcurrency: 1\n",
            "  revision: openaq-ingest-00009-bup\n",
            "  service: projects/mgmt-467-nh/locations/us-central1/services/openaq-ingest\n",
            "  serviceAccountEmail: 16331834856-compute@developer.gserviceaccount.com\n",
            "  timeoutSeconds: 60\n",
            "  uri: https://openaq-ingest-j2xybhwaka-uc.a.run.app\n",
            "state: ACTIVE\n",
            "updateTime: '2025-12-05T03:11:46.229466632Z'\n",
            "url: https://us-central1-mgmt-467-nh.cloudfunctions.net/openaq-ingest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2.4: Trigger Ingest & Validate Streaming Data\n",
        "import requests\n",
        "import time\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# 1. Trigger the Cloud Function\n",
        "# We retrieve the URL dynamically from gcloud to make it robust\n",
        "function_url = !gcloud functions describe openaq-ingest --gen2 --region=$region --format=\"value(url)\"\n",
        "function_url = function_url[0]\n",
        "\n",
        "print(f\"Triggering Cloud Function at: {function_url}\")\n",
        "response = requests.get(function_url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    print(f\"Function Response: {response.text}\")\n",
        "else:\n",
        "    print(f\"Error triggering function: {response.text}\")\n",
        "\n",
        "# 2. Wait for Ingestion\n",
        "print(\"Waiting 15 seconds for data to flow from Pub/Sub to BigQuery...\")\n",
        "time.sleep(15)\n",
        "\n",
        "# 3. Validate Data in BigQuery\n",
        "print(\"Querying BigQuery for latest streaming records...\")\n",
        "\n",
        "query = f\"\"\"\n",
        "    SELECT timestamp, city, parameter, value, unit\n",
        "    FROM `{project_id}.{dataset_name}.{streaming_table_name}`\n",
        "    ORDER BY timestamp DESC\n",
        "    LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "query_job = bq_client.query(query)\n",
        "results = list(query_job.result())\n",
        "\n",
        "if results:\n",
        "    print(f\"\\nSUCCESS: Found {len(results)} records.\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'TIMESTAMP':<25} | {'CITY':<20} | {'PARAM':<10} | {'VALUE'}\")\n",
        "    print(\"-\" * 60)\n",
        "    for row in results:\n",
        "        print(f\"{str(row.timestamp):<25} | {row.city[:20]:<20} | {row.parameter:<10} | {row.value}\")\n",
        "else:\n",
        "    print(\"\\nWARNING: No rows found yet. The pipeline might need a few more seconds.\")\n",
        "    print(\"Try running this cell again in 1 minute.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGE8rAy4lDQa",
        "outputId": "c37d9d14-aecd-49b8-faf8-a363c04a08b7"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Triggering Cloud Function at: https://us-central1-mgmt-467-nh.cloudfunctions.net/openaq-ingest\n",
            "Function Response: Success: Published 3 live readings from Rome.\n",
            "Waiting 15 seconds for data to flow from Pub/Sub to BigQuery...\n",
            "Querying BigQuery for latest streaming records...\n",
            "\n",
            "SUCCESS: Found 3 records.\n",
            "------------------------------------------------------------\n",
            "TIMESTAMP                 | CITY                 | PARAM      | VALUE\n",
            "------------------------------------------------------------\n",
            "2025-12-05 04:00:00+00:00 | Rome                 | o3         | 4.0\n",
            "2025-12-05 04:00:00+00:00 | Rome                 | co         | 422.0\n",
            "2025-12-05 04:00:00+00:00 | Rome                 | no2        | 28.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3 - Analytics and Modeling:**"
      ],
      "metadata": {
        "id": "Gogu0-ofhAHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 3.1 (Revised): Retrain BQML Model (Unit-Corrected)\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Feature Engineering in SQL:\n",
        "# 1. CO_GT_ * 1000 -> Converts mg/m^3 to µg/m^3 (Matching Open-Meteo)\n",
        "# 2. NO2_GT_       -> Already µg/m^3 (Matching Open-Meteo)\n",
        "# 3. Removing O3   -> Removing the incompatible feature\n",
        "\n",
        "train_query = f\"\"\"\n",
        "    CREATE OR REPLACE MODEL `{project_id}.{dataset_name}.air_quality_model_v2`\n",
        "    OPTIONS(model_type='LINEAR_REG') AS\n",
        "    SELECT\n",
        "        (CO_GT_ * 1000) as label,   -- TARGET: Converted to Micrograms\n",
        "        NO2_GT_ as no2              -- FEATURE: Nitrogen Dioxide (True Concentration)\n",
        "    FROM\n",
        "        `{project_id}.{dataset_name}.{table_name}`\n",
        "    WHERE\n",
        "        CO_GT_ > -200\n",
        "        AND NO2_GT_ > -200\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Training Engineered Model (v2) in {project_id}.{dataset_name}...\")\n",
        "job = bq_client.query(train_query)\n",
        "job.result()\n",
        "\n",
        "print(\"SUCCESS: Unit-Corrected Model trained.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EQ82ZGchE8A",
        "outputId": "e6b766c0-e448-4f71-9b6e-791046becbb3"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Engineered Model (v2) in mgmt-467-nh.air_quality_dataset...\n",
            "SUCCESS: Unit-Corrected Model trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 3.2: Evaluate Model Performance\n",
        "eval_query = f\"\"\"\n",
        "    SELECT *\n",
        "    FROM ML.EVALUATE(MODEL `{project_id}.{dataset_name}.air_quality_model`, (\n",
        "        SELECT\n",
        "            CO_GT_ as label,\n",
        "            NO2_GT_ as no2,\n",
        "            PT08_S5_O3_ as o3\n",
        "        FROM\n",
        "            `{project_id}.{dataset_name}.{table_name}`\n",
        "        WHERE\n",
        "            CO_GT_ > -200 AND NO2_GT_ > -200 AND PT08_S5_O3_ > -200\n",
        "    ))\n",
        "\"\"\"\n",
        "\n",
        "print(\"Running Model Evaluation...\")\n",
        "job = bq_client.query(eval_query)\n",
        "results = list(job.result())\n",
        "\n",
        "print(\"\\n--- Model Metrics ---\")\n",
        "for row in results:\n",
        "    print(f\"R2 Score: {row.r2_score:.4f}\")\n",
        "    print(f\"Mean Absolute Error: {row.mean_absolute_error:.4f} mg/m^3\")\n",
        "    print(f\"Mean Squared Error: {row.mean_squared_error:.4f}\")\n",
        "\n",
        "print(\"\\nInterpretation: An R2 close to 1.0 is perfect. An R2 of 0 means the model is useless.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQKeGZM9hyEr",
        "outputId": "a7562bd1-c652-496c-9834-51744c7ee14a"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Model Evaluation...\n",
            "\n",
            "--- Model Metrics ---\n",
            "R2 Score: 0.7393\n",
            "Mean Absolute Error: 0.5350 mg/m^3\n",
            "Mean Squared Error: 0.5414\n",
            "\n",
            "Interpretation: An R2 close to 1.0 is perfect. An R2 of 0 means the model is useless.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 3.3 (Revised): Predict with Corrected Model\n",
        "predict_query = f\"\"\"\n",
        "    WITH live_features AS (\n",
        "        SELECT\n",
        "            timestamp,\n",
        "            -- We only need NO2 now\n",
        "            MAX(CASE WHEN parameter = 'no2' THEN value END) as no2,\n",
        "            MAX(CASE WHEN parameter = 'co' THEN value END) as actual_co_live\n",
        "        FROM `{project_id}.{dataset_name}.{streaming_table_name}`\n",
        "        GROUP BY timestamp\n",
        "        HAVING no2 IS NOT NULL\n",
        "    )\n",
        "\n",
        "    SELECT\n",
        "        timestamp,\n",
        "        ROUND(predicted_label, 2) as predicted_co_ug, -- Prediction in Micrograms\n",
        "        actual_co_live,\n",
        "        no2 as input_no2\n",
        "    FROM\n",
        "        ML.PREDICT(MODEL `{project_id}.{dataset_name}.air_quality_model_v2`,\n",
        "        (SELECT * FROM live_features))\n",
        "    ORDER BY timestamp DESC\n",
        "    LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "print(\"Running Predictions using Engineered Model...\")\n",
        "job = bq_client.query(predict_query)\n",
        "results = list(job.result())\n",
        "\n",
        "print(\"\\n--- Live Predictions (Corrected Scale) ---\")\n",
        "print(f\"{'TIMESTAMP':<25} | {'PREDICTED CO (µg)':<18} | {'ACTUAL CO':<10} | {'INPUT NO2'}\")\n",
        "print(\"-\" * 75)\n",
        "for row in results:\n",
        "    print(f\"{str(row.timestamp):<25} | {row.predicted_co_ug:<18} | {row.actual_co_live:<10} | {row.input_no2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlhN2UddiGIC",
        "outputId": "1c77f083-db53-4c71-a1c1-3664425f3c1e"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Predictions using Engineered Model...\n",
            "\n",
            "--- Live Predictions (Corrected Scale) ---\n",
            "TIMESTAMP                 | PREDICTED CO (µg)  | ACTUAL CO  | INPUT NO2\n",
            "---------------------------------------------------------------------------\n",
            "2025-12-05 04:00:00+00:00 | 430.88             | 422.0      | 28.5\n"
          ]
        }
      ]
    }
  ]
}